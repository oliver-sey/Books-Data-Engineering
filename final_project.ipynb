{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books Data Engineering ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spring 2024, by Oliver Seymour\n",
    "\n",
    "\n",
    "#### My Data Sources:\n",
    "\n",
    "Goodreads books and reviews: https://www.kaggle.com/datasets/jealousleopard/goodreadsbooks\n",
    "\n",
    "Audible books and reviews: https://www.kaggle.com/code/satyanarayanam/cleaning-audible-dataset/input\n",
    "\n",
    "***add another here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: uncomment these!!\n",
    "# %pip install pandas\n",
    "# %pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import my Google Books API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the .env file to get my API key for Google books\n",
    "# I tried to use the dotenv library but the install didn't work\n",
    "\n",
    "with open('.env', 'r') as file:\n",
    "\tfor line in file:\n",
    "\t\t# we want to skip empty lines or lines that are comments\n",
    "\t\tif line.strip() == '' or line.startswith('#'):\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\t# strip each like and then split it to make a key-value pair\n",
    "\t\tkey, value = line.strip().split('=', 1)\n",
    "\t\t\n",
    "\t\t# set the env variable\n",
    "\t\tos.environ[key] = value\n",
    "\n",
    "# set the API key variable so I can use it later\n",
    "GOOGLE_BOOKS_API_KEY = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on the import:\n",
    "\n",
    "I was getting errors when importing goodreads.csv (the dataset on books and reviews from Goodreads), related to finding the wrong number of columns.\n",
    "\n",
    "I manually fixed row 3350 where a comma before 'Jr.' in an author's name was causing issues since a comma is the delimiter\n",
    "\n",
    "Fixed a similar comma issue in row 4704\n",
    "\n",
    "Removed a comma in the middle of \"James Wesley Rawles\" (the only author's name) on row 5879\n",
    "\n",
    "Fixed a similar issue in row 8981 - changed author column from 'James Brown, Son & Ferguson' to 'James Brown and Son & Ferguson'. I know this is a pretty manual edit of the data, but I felt like it didn't make sense for this to get split into the three authors 'James Brown', 'Son', and 'Ferguson'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audible_df = pd.read_csv('Data/audible_copy_pasted.csv', encoding='utf-8')\n",
    "print(\"data from audible:\")\n",
    "display(audible_df.head())\n",
    "\n",
    "goodreads_df = pd.read_csv('Data/goodreads_copy_pasted.csv', sep=',', encoding='utf-8', on_bad_lines='warn')\n",
    "print(\"data from goodreads:\")\n",
    "display(goodreads_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check that there are no character encoding issues on a couple rows that had problems before\n",
    "display(audible_df[(audible_df['time'] == '9 hrs and 17 mins') & (audible_df['language'] == 'mandarin_chinese')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Goodreads dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shape?\n",
    "goodreads_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Audible dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shape?\n",
    "audible_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many NAs are there?\n",
    "audible_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if the author column always starts with 'Writtenby:'\n",
    "display(audible_df[~audible_df['author'].str.contains(pat=r'^Writtenby:', regex=True)])\n",
    "# wow ok, there are 0 rows that don't have that starting, that will be easy to clean\n",
    "\n",
    "# and if the narrator column always starts with 'Narratedby:'\n",
    "display(audible_df[~audible_df['narrator'].str.contains(pat=r'^Narratedby:', regex=True)])\n",
    "# ok nice, once again they all follow that pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we know these columns always start with 'Writtenby:' and 'Narratedby:', how can we split up the first and last names?\n",
    "# regex: first 'Writtenby: and then either one or more of: (a capital letter followed by one or more lowercase letters), \n",
    "# basically just FirstNameOrMaybeMoreName\n",
    "# or (one or more of: (a capital letter followed by a period), i.e. for an initial, followed by a capital letter and then one or more lowercase\n",
    "# basically A.B.LastNameMaybeMore\n",
    "\n",
    "\n",
    "# display(audible_df[~audible_df[\"author\"].str.contains(pat=r\"^Writtenby:( (([A-Z][a-z]+)+ | (([A-Z][.])+[A-Z][a-z]+))+[,]* )\")])\n",
    "# display(audible_df[audible_df[\"author\"].str.contains(pat=r\"^Writtenby:((([A-Z][.])+[A-Z][a-z]+)+[,]*)\")])\n",
    "display(audible_df[~audible_df[\"author\"].str.contains(pat=r\"^Writtenby:([A-Z][a-z]+)+\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check for inconsistencies in the formatting of the 'time' column\n",
    "# do they all have 'and'?\n",
    "display(audible_df[~audible_df['time'].str.contains(pat=r'and', regex=True)].head())\n",
    "# ok not all do\n",
    "\n",
    "# do they all follow the <numbers><space><letters> etc. pattern?\n",
    "display(audible_df[~audible_df['time'].str.contains(pat=r'^\\d+\\s+[A-Za-z]+')].head())\n",
    "\n",
    "# let's get the unique values:\n",
    "audible_df.loc[~audible_df['time'].str.contains(r'^\\d+\\s+[A-Za-z]+', regex=True), 'time'].unique()\n",
    "# ok seems like it always contains 'and', except if the value is 'Less than 1 minute'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the stars column\n",
    "# let's see what values there are:\n",
    "audible_df['stars'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How often is the stars column 'Not rated yet', or some other string that doesn't contain a digit?\n",
    "display(audible_df[~audible_df['stars'].str.contains(r'\\d')].shape)\n",
    "# wow that's a lot of books that aren't rated yet!\n",
    "display(audible_df[~audible_df['stars'].str.contains(r'\\d')])\n",
    "\n",
    "# what unique values are there?\n",
    "audible_df[~audible_df['stars'].str.contains(r'\\d')]['stars'].unique()\n",
    "# ok seems to just be 'Not rated yet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the values in 'stars' all follow the same pattern?\n",
    "print(\"check that it's always out of 5 stars:\")\n",
    "display(audible_df[~(audible_df['stars'].str.contains(r'out of 5 stars') \n",
    "\t\t\t\t\t | audible_df['stars'].str.contains(r'Not rated yet'))]['stars'].unique())\n",
    "\n",
    "print(\"check that its always either an integer number of stars, or 1 decimal digit, or Not rated yet:\")\n",
    "# at the start of the string, one digit, then an optional period, then an optional digit, then it has to be a space\n",
    "display(audible_df[~(audible_df['stars'].str.contains(r'^\\d[.]*\\d*\\s') \n",
    "\t\t\t\t\t | audible_df['stars'].str.contains(r'Not rated yet'))]['stars'].unique())\n",
    "\n",
    "print(\"values in 'stars' where they don't follow the 'X.X out of 5 starsXXX (1-3 digits) rating[maybe an 's']' or 'Not rated yet' pattern:\")\n",
    "display(audible_df[~(audible_df['stars'].str.contains(r'\\d[.]*\\d* out of 5 stars\\d{1,3} rating[s]*') \n",
    "\t\t\t\t\t | audible_df['stars'].str.contains(r'Not rated yet'))]['stars'].unique())\n",
    "\n",
    "print(\"also excluding e.g. '4.5 out of 5 stars1,869 ratings', with the comma in the ratings count:\")\n",
    "display(audible_df[~(audible_df['stars'].str.contains(r'\\d[.]*\\d* out of \\d stars\\d{1,3} rating[s]*') \n",
    "\t\t\t\t\t | audible_df['stars'].str.contains(r'Not rated yet')\n",
    "\t\t\t\t\t | audible_df['stars'].str.contains(r'\\d[.]*\\d* out of \\d stars\\d+[,]\\d+ rating[s]*')\n",
    "\t\t\t\t\t )]['stars'].unique())\n",
    "\n",
    "# ok it seems like we only have a few possible patterns in this column\n",
    "# and the stars are always out of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the release date column always follow the pattern <numbers> - <numbers> - <numbers>?\n",
    "display(audible_df[~audible_df['releasedate'].str.contains(r'^\\d+-\\d+-\\d+', regex=True)])\n",
    "# Ok nice they all follow that pattern\n",
    "\n",
    "display(audible_df[~audible_df['releasedate'].str.contains(r'^\\d{2}-\\d{2}-\\d{4}', regex=True)])\n",
    "# but it appears they don't always follow DD-MM-YYYY, sometimes it is D-M-YYYY, and sometimes it is just YY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what values are there in language?\n",
    "display(audible_df['language'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the ****third dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Audible dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a deep copy\n",
    "cleaned_audible_df = audible_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the price column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's clean up the price column and get it into a float\n",
    "# display(cleaned_audible_df[cleaned_audible_df['cleaned_price'].str.contains(',')])\n",
    "\n",
    "# since there are commas in the numbers, e.g. 1,234.00, we will have to get rid of commas\n",
    "cleaned_audible_df['price_no_commas'] = cleaned_audible_df['price'].str.replace(',', '')\n",
    "\n",
    "# check if the comma removing worked correctly:\n",
    "# .option_context(\"display.min_rows\", 10000):\n",
    "print('rows where price contains a comma:')\n",
    "display(cleaned_audible_df[cleaned_audible_df['price'].str.contains(',')])\n",
    "\n",
    "print(\"Check the NA's:\")\n",
    "display(cleaned_audible_df.isna().sum())\n",
    "# display(cleaned_audible_df[cleaned_audible_df['price_no_commas'].isna()])\n",
    "\n",
    "print(\"rows where price has a non-zero digit after the decimal place:\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['price_no_commas'].str.contains(r'[.][1-9]', regex=True, na=False)])\n",
    "# ok there are some prices that have a fraction, so we can't just make them ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what unique values are in the price column besides just digits\n",
    "cleaned_audible_df[cleaned_audible_df['price'].str.contains(r'[A-Za-z]', regex=True)]['price'].unique()\n",
    "# Ok nice only 'Free', let's make those 0\n",
    "\n",
    "cleaned_audible_df['cleaned_price'] = cleaned_audible_df['price_no_commas'].apply(lambda value: 0 if value == 'Free' else value)\n",
    "\n",
    "# let's also convert to a float\n",
    "cleaned_audible_df['cleaned_price'] = cleaned_audible_df['cleaned_price'].astype(float)\n",
    "\n",
    "display(cleaned_audible_df.head())\n",
    "\n",
    "display(cleaned_audible_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "# with pd.option_context(\"display.min_rows\", 10000):\n",
    "display(cleaned_audible_df)\n",
    "\n",
    "# looks good, now we can drop the other columns and rename cleaned_price\n",
    "cleaned_audible_df.drop(columns=['price', 'price_no_commas'], inplace=True)\n",
    "cleaned_audible_df.rename(columns= {'cleaned_price' : 'cleaned_price_rupees'}, inplace=True)\n",
    "\n",
    "print(\"After dropping and renaming:\")\n",
    "display(cleaned_audible_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: !!!!! have to deal with weird character encoding issues in the audible dataset as well\n",
    "# like ç¬¬äºŒåäº”è©±ã‚µãƒ³ãƒ»ãƒŸã‚·ã‚§ãƒ«ã®ã„ã„ã...\tWrittenby:æ£®æœ¬å“²éƒŽ\tNarratedby:å°é‡Žç”°è‹±ä¸€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the author and narrator columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my exploration I found that every row in the author and narrator column started with\n",
    "# 'Writtenby:' and 'Narratedby:', so let's remove that\n",
    "\n",
    "# replace 'Writtenby:' with an empty string\n",
    "cleaned_audible_df['author'] = cleaned_audible_df['author'].str.replace(pat=r'^Writtenby:', repl='', regex=True)\n",
    "\n",
    "# replace 'Narrattedby:' with an empty string\n",
    "cleaned_audible_df['narrator'] = cleaned_audible_df['narrator'].str.replace(pat=r'^Narratedby:', repl='', regex=True)\n",
    "\n",
    "display(cleaned_audible_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the releasedate column (converting to timestamp):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the release date to a datetime object\n",
    "# **Important note, they seem to be in a DD-MM-YYYY (or sometimes D-M-YY, etc. but day first) format, \n",
    "# since there are some dates like 30-10-18, 25-11-14 (so the day must be first), but there is also 1-5-2018\n",
    "cleaned_audible_df['cleaned_releasedate'] = pd.to_datetime(cleaned_audible_df['releasedate'], dayfirst=True, format='mixed')\n",
    "\n",
    "# temporarily display a lot of rows to check it worked\n",
    "with pd.option_context(\"display.min_rows\", 10):\n",
    "\t# look at just rows where releasedate had a weird format to especially make sure it worked for those\n",
    "\tdisplay(cleaned_audible_df[~cleaned_audible_df['releasedate'].str.contains(r'\\d{2}-\\d{2}-\\d{4}', regex=True)])\n",
    "\tdisplay(cleaned_audible_df)\n",
    "\n",
    "# ok seems to have worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure there are no weird values in cleaned_releasedate\n",
    "print(\"min release date values:\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['cleaned_releasedate'] == cleaned_audible_df['cleaned_releasedate'].min()])\n",
    "# ok the min values look reasonable\n",
    "\n",
    "# check books that were released after today's date\n",
    "print(\"release date values that are after today's date:\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['cleaned_releasedate'] > pd.to_datetime('today').normalize()])\n",
    "# I don't think this is a mistake since one of the original releasedate values is '9-8-2024'\n",
    "# These have very weird character encoding issues and I don't think it makes sense to have books that aren't release yet\n",
    "# so I'm going to drop these\n",
    "\n",
    "cleaned_audible_df = cleaned_audible_df[cleaned_audible_df['cleaned_releasedate'] <= pd.to_datetime('today').normalize()]\n",
    "print(\"new max release date value:\")\n",
    "display(cleaned_audible_df['cleaned_releasedate'].max())\n",
    "# ok nice, now the max value was released about 2 weeks ago, that seems reasonable\n",
    "\n",
    "# now let's drop the releasedate column\n",
    "cleaned_audible_df.drop(columns=['releasedate'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the time column and extracting hours and mins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt at converting the time column by splitting it into hours, mins, etc. components in different columns and then combining again\n",
    "print(\"df with the hours extracted:\")\n",
    "display(cleaned_audible_df['time'].str.extract(pat=r'([A-Za-z]+\\s*)+')[0].unique())\n",
    "\n",
    "# at the start of the string, get one or more digits, then a white space, hr, maybe an s\n",
    "# but only keep the digits\n",
    "# i.e. we get '8 hrs' or '1 hr' and we only keep the 8 or 1\n",
    "cleaned_audible_df['hours'] = cleaned_audible_df['time'].str.extract(pat=r'^(\\d+)\\shr[s]*')\n",
    "# seems to have worked\n",
    "# we have an NA if it is 0 hours, so let's fill the NAs with 0's\n",
    "cleaned_audible_df['hours'] = cleaned_audible_df['hours'].fillna(0)\n",
    "\n",
    "# with pd.option_context(\"display.min_rows\", 10000):\n",
    "display(cleaned_audible_df)\n",
    "\n",
    "# check that it worked for when it is just '1 hr', not hrs\n",
    "print(\"rows with time == '1 hr':\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['time'] == '1 hr'])\n",
    "\n",
    "# check that the fillNA worked correctly\n",
    "print(\"rows with 0 in the hours column:\")\n",
    "# with pd.option_context(\"display.min_rows\", 10000):\n",
    "display(cleaned_audible_df[cleaned_audible_df['hours'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the minutes\n",
    "# cleaned_audible_df['minutes'] = cleaned_audible_df['time'].str.extract(r'')\n",
    "cleaned_audible_df[cleaned_audible_df['time'].str.contains(r'min$', regex=True)]\n",
    "# ok it seems we have to be careful with not getting 'Less than 1 minute' and something like '1 hr and 1 min' mixed up\n",
    "\n",
    "# one or more digits, then 1 whitespace, then 'min', then maybe an s, all this at the end of the string\n",
    "# only capture the digits though\n",
    "# it is important that it's at the end of the string so we don't accidentally capture the 1 from 'Less than 1 minute', \n",
    "# we only want the number when the string ends in min or mins\n",
    "print(\"df with minutes extracted:\")\n",
    "cleaned_audible_df['minutes'] = cleaned_audible_df['time'].str.extract(r'(\\d+)\\smin[s]*$')\n",
    "# cleaned_audible_df[cleaned_audible_df['time'].str.contains(r'(\\d+\\smin[s]*$)')]\n",
    "\n",
    "# with pd.option_context(\"display.min_rows\", 10000):\n",
    "display(cleaned_audible_df)\n",
    "\n",
    "# check that it correctly did NA for rows where there are no minutes\n",
    "print(\"rows with NA in the minutes column:\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['minutes'].isna()])\n",
    "\n",
    "# the minutes column seems to have worked nicely, let's fill in the NAs with 0's\n",
    "cleaned_audible_df['minutes'] = cleaned_audible_df['minutes'].fillna(0)\n",
    "\n",
    "print(\"cleaned_audible_df with minutes NAs filled in with 0's:\")\n",
    "# with pd.option_context(\"display.min_rows\", 10000):\n",
    "display(cleaned_audible_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the columns to integer\n",
    "cleaned_audible_df['hours'] = cleaned_audible_df['hours'].astype(int)\n",
    "cleaned_audible_df['minutes'] = cleaned_audible_df['minutes'].astype(int)\n",
    "\n",
    "cleaned_audible_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many rows are only 1 minute, to help us figure out what to do with the 'Less than 1 minute'\n",
    "cleaned_audible_df[(cleaned_audible_df['hours'] == 0) & (cleaned_audible_df['minutes'] == 1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what to do with rows where it is 'Less than 1 minute'????\n",
    "# let's see what unique values there are in the time column where we got NA for both hours and mins, and filled them both in with 0's:\n",
    "display(cleaned_audible_df[(cleaned_audible_df['hours'] == 0) & (cleaned_audible_df['minutes'] == 0)]['time'].unique())\n",
    "# Ok, just 'Less than 1 minute'. I will have to figure out how to take care of this\n",
    "\n",
    "# how many are there?\n",
    "print(\"# of rows where time is 'Less than 1 minute':\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['time'] == 'Less than 1 minute'].shape[0])\n",
    "\n",
    "# ok since it is just 61 in 88k, and since we don't have any precision smaller than minutes, \n",
    "# I think we should just round this to 1 minute\n",
    "cleaned_audible_df.loc[cleaned_audible_df['time'] == 'Less than 1 minute', 'minutes'] = 1\n",
    "\n",
    "print(\"rows where 'time' is 'Less than 1 minute':\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['time'] == 'Less than 1 minute'].head())\n",
    "\n",
    "# ok looks good, now that we have a nice hours and minutes column, we can convert that to a timedelta and drop 'time'\n",
    "# cleaned_audible_df.drop(columns=['time'], inplace=True)\n",
    "\n",
    "# convert the hours and minutes columns to separate columns that are a pandas time delta\n",
    "cleaned_audible_df['hours_timedelta'] = pd.to_timedelta(cleaned_audible_df['hours'], unit='h')\n",
    "cleaned_audible_df['minutes_timedelta'] = pd.to_timedelta(cleaned_audible_df['minutes'], unit='m')\n",
    "\n",
    "# add the two timedelta columns together to get the full runtime\n",
    "cleaned_audible_df['cleaned_runtime'] = cleaned_audible_df['hours_timedelta'] + cleaned_audible_df['minutes_timedelta']\n",
    "\n",
    "print(\"With timedelta columns:\")\n",
    "display(cleaned_audible_df)\n",
    "\n",
    "# drop the unnecessary columns, including time\n",
    "cleaned_audible_df.drop(columns=['time', 'hours', 'minutes', 'hours_timedelta', 'minutes_timedelta'], inplace=True)\n",
    "print(\"With columns dropped:\")\n",
    "display(cleaned_audible_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the stars column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok we know there are only a few patterns that values in this column follow\n",
    "# at the start of the string, get a number and then optionally a decimal and another number\n",
    "# (we know from the exploration that it's only ever an integer for the stars, or one decimal place)\n",
    "cleaned_audible_df['star_count'] = cleaned_audible_df['stars'].str.extract(r'^(\\d[.]*\\d*)')\n",
    "\n",
    "cleaned_audible_df\n",
    "# ok we have NAs in rows with no ratings yet but I think that actually makes sense as a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_audible_df['ratings_count'] = cleaned_audible_df['stars'].str.extract(r'([\\d,]+) rating')\n",
    "\n",
    "display(cleaned_audible_df.isna().sum())\n",
    "# check that it worked\n",
    "print(\"df after making ratings_count:\")\n",
    "display(cleaned_audible_df.head())\n",
    "\n",
    "# check that it worked when there is a comma in the ratings number (e.g. 1,400 ratings)\n",
    "print(\"rows with a comma in stars:\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['stars'].str.contains(',')].head())\n",
    "\n",
    "\n",
    "# for when there are no ratings, I think it makes sense to have ratings_count be 0, and not NA\n",
    "# but star_count should be NA I think since 0 stars means something different from no reviews\n",
    "cleaned_audible_df['ratings_count'] = cleaned_audible_df['ratings_count'].fillna(0)\n",
    "\n",
    "# check that it worked when there are no ratings\n",
    "print(\"rows where stars is 'Not rated yet':\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['stars'].str.contains('Not rated yet')].head())\n",
    "\n",
    "# ok looks good, now let's remove commas from the ratings_count\n",
    "cleaned_audible_df['ratings_count'] = cleaned_audible_df['ratings_count'].str.replace(',', '')\n",
    "# another fillna?? since I was getting weird behavior\n",
    "cleaned_audible_df['ratings_count'] = cleaned_audible_df['ratings_count'].fillna(0)\n",
    "\n",
    "print(\"After removing commas from ratings_count:\")\n",
    "display(cleaned_audible_df.head())\n",
    "print(\"check rows with commas one last time:\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['stars'].str.contains(',')].head())\n",
    "# display(cleaned_audible_df[cleaned_audible_df['ratings_count'].str.contains('^0', regex=True, na=True)].head())\n",
    "\n",
    "print(\"check NAs:\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['ratings_count'].isna()].head())\n",
    "\n",
    "display(cleaned_audible_df.isna().sum())\n",
    "\n",
    "# ok looks good, now we can drop the stars column\n",
    "cleaned_audible_df.drop(columns=['stars'], inplace=True)\n",
    "\n",
    "# convert the star_count to float, and ratings_count to int\n",
    "cleaned_audible_df['star_count'] = cleaned_audible_df['star_count'].astype(float)\n",
    "cleaned_audible_df['ratings_count'] = cleaned_audible_df['ratings_count'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a unique ID in each row of the Audible DF\n",
    "# Even though I know we will need to drop some rows after this, I think this is a good time to do this\n",
    "\n",
    "# I think name (title of the book) is a good column to use for this\n",
    "# first just as a check, make sure we have only unique values in 'name'\n",
    "\n",
    "# wow ok we do have duplicates by book name\n",
    "# there are even duplicates when you group together the name, author, and narrator\n",
    "# and 179 duplicates when you group together the name, author, narrator, language, and the release date\n",
    "\t# I think this last combination doesn't make sense for there to be duplicates\n",
    "\t# even if there are different runtimes of the audiobooks, I don't think it makes sense\n",
    "\t# to have these duplicates, and it is not a lot of rows\n",
    "\n",
    "# duplicates = cleaned_audible_df.duplicated(subset=['name', 'author', 'narrator', 'language', 'cleaned_releasedate'])\n",
    "# display(cleaned_audible_df[duplicates])\n",
    "\n",
    "display(len(cleaned_audible_df))\n",
    "display(len(cleaned_audible_df.drop_duplicates(subset=['name', 'author', 'narrator', 'language', 'cleaned_releasedate'])))\n",
    "# ok looks like this will only remove the 179 rows, like expected\n",
    "# let's drop them\n",
    "cleaned_audible_df.drop_duplicates(subset=['name', 'author', 'narrator', 'language', 'cleaned_releasedate'], inplace=True)\n",
    "\n",
    "# now that we have done that dropping, I feel like each row is ok to get assigned a unique ID\n",
    "# since each row should be unique at this point, and I would either need to use 1 unique column\n",
    "# for pd.factorize(), which we don't really have, or do something complicated to combine multiple\n",
    "# columns to make a unique combination, I am just going to use each row's index\n",
    "cleaned_audible_df['audible_ID'] = cleaned_audible_df.index\n",
    "\n",
    "cleaned_audible_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Goodreads dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clean here!!!!\n",
    "cleaned_goodreads_df = goodreads_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A final check of the cleaned dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"goodreads:\")\n",
    "display(cleaned_goodreads_df.head())\n",
    "\n",
    "print(\"audible:\")\n",
    "display(cleaned_audible_df.head())\n",
    "\n",
    "print(\"other dataset: *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing google books API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python's built-in module for encoding and decoding JSON data\n",
    "import json\n",
    "\n",
    "# Python's built-in module for opening and reading URLs\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "# for converting strings to a format that works for URLs\n",
    "from urllib.parse import quote\n",
    "\n",
    "books_found_not_perfect_match = 0\n",
    "books_found_perfect_match = 0\n",
    "books_not_found = 0\n",
    "\n",
    "\n",
    "# def google_books_query(title, author):\n",
    "def google_books_query(ISBN):\n",
    "\t# use the global version of these variables\n",
    "\tglobal books_found_not_perfect_match\n",
    "\tglobal books_found_perfect_match\n",
    "\tglobal books_not_found\n",
    "\n",
    "\t# converted_title = convert_string_for_url(title)\n",
    "\t# converted_author = convert_string_for_url(author)\n",
    "\n",
    "\t# my version:\n",
    "\t# create getting started variables\n",
    "\t# url = (\n",
    "\t#\t \"https://www.googleapis.com/books/v1/volumes?q=intitle:\"\n",
    "\t#\t + converted_title\n",
    "\t#\t + \"inauthor:\"\n",
    "\t#\t + converted_author\n",
    "\t#\t + \"&key=\"\n",
    "\t#\t+ GOOGLE_BOOKS_API_KEY\n",
    "\t\n",
    "\n",
    "\turl = (\n",
    "\t\t\"https://www.googleapis.com/books/v1/volumes?q=isbn:\"\n",
    "\t\t+ ISBN\n",
    "\t + \"&key=\"\n",
    "\t\t+ GOOGLE_BOOKS_API_KEY\n",
    "\t)\n",
    "\t# param = input(\"Enter query: \").strip()\n",
    "\n",
    "\t# send a request and get a JSON response\n",
    "\tresp = urlopen(url)\n",
    "\n",
    "\t# parse JSON into Python as a dictionary\n",
    "\tbook_data = json.load(resp)\n",
    "\t# print(\"resp:\", resp)\n",
    "\t# print(\"book_data:\", book_data)\n",
    "\ttry:\n",
    "\t\tif \"items\" in book_data:\n",
    "\t\t\t# create additional variables for easy querying\n",
    "\t\t\tvolume_info = book_data[\"items\"][0][\"volumeInfo\"]\n",
    "\t\t\tauthor = volume_info[\"authors\"]\n",
    "\t\t\t# practice with conditional expressions!\n",
    "\t\t\tprettify_author = author if len(author) > 1 else author[0]\n",
    "\n",
    "\t\t\t# print(\"query title:\", title)\n",
    "\t\t\t# print(\"found title:\", volume_info['title'])\n",
    "\n",
    "\t\t # print(\"query author:\", author)\n",
    "\t\t\t# print(\"found author:\", prettify_author)\n",
    "\n",
    "\t\t\t# perfect match\n",
    "\t\t\t# if title == volume_info[\"title\"]:\n",
    "\t\t\t#\t books_found_perfect_match += 1\n",
    "\t\t\t# else:\n",
    "\t\t\t#\t books_found_not_perfect_match += 1\n",
    "\n",
    "\t\t\t# display title, author, page count, publication date\n",
    "\t\t    # fstrings require Python 3.6 or higher\n",
    "\t\t\t# \\n adds a new line for easier reading\n",
    "\t\t\t# print(f\"\\nTitle: {volume_info['title']}\")\n",
    "\t\t\t# print(f\"Author: {prettify_author}\")\n",
    "\t\t\t# print(f\"Page Count: {volume_info['pageCount']}\")\n",
    "\t\t\t# print(f\"Publication Date: {volume_info['publishedDate']}\")\n",
    "\n",
    "\t\t\t# print(volume_info)\n",
    "\n",
    "\t\t# else:\n",
    "\t\t\t# print(\"******\\n\\n\\n\\n\\nno results found for title:\", title, \"author:\", author)\n",
    "\t\t\t# books_not_found += 1\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"***\\n\\n\\nError:\", e)\n",
    "\n",
    "\t# print(\"\\n***\\n\")\n",
    "\n",
    "\treturn book_data\n",
    "\n",
    "\n",
    "def convert_string_for_url(input_string):\n",
    "\n",
    "\t# Encode the string for URL\n",
    "\tencoded_string = quote(input_string)\n",
    "\n",
    "\treturn encoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book_data = google_books_query(\"Legacy\", \"Shannon Messenger\")\n",
    "\n",
    "# for item in book_data['items']:\n",
    "# \tprint(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in cleaned_audible_df.head(100).iterrows():\n",
    "# \tname = row[\"name\"]\n",
    "# \tauthor = row[\"author\"]\n",
    "\n",
    "# \tbook_data = google_books_query(name, author)\n",
    "# \tprint(book_data)\n",
    "# \tprint(\"query title:\", title)\n",
    "# \tprint(\"query author:\", author)\n",
    "\t\n",
    "# \tfor item in book_data[\"items\"]:\n",
    "# \t\tif \"volumeInfo\" in item:\n",
    "# \t\t\tif title == item['volumeInfo'][\"title\"]:\n",
    "# \t\t\t\tprint(\"exact title match\")\n",
    "# \t\t\t\tbreak\n",
    "\n",
    "# \t\t\tprint(\"found title:\", item['volumeInfo'][\"title\"])\n",
    "# \t\t\tprint(\"found author:\", item['volumeInfo'][\"authors\"])\n",
    "\n",
    "\t\t\t\n",
    "# \t\telse:\n",
    "# \t\t\tprint(\"didn't find volume info\")\n",
    "# \tprint(book_data)\n",
    "\n",
    "# \tprint(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"books_found_not_perfect_match:\", books_found_not_perfect_match)\n",
    "# print(\"books_found_perfect_match:\", books_found_perfect_match)\n",
    "# print(\"books_not_found:\", books_not_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attempt at making a really quick and dirty df from google books data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a test for now\n",
    "test_integrated_df = goodreads_df.merge(audible_df, left_on='title', right_on='name', how='outer', indicator=True)\n",
    "# test_integrated_df = audible_df.merge(goodreads_df, left_on='title', right_on='name')\n",
    "\n",
    "display(len(test_integrated_df[test_integrated_df['_merge'] == 'both']))\n",
    "\n",
    "audible_goodreads_merged = test_integrated_df[test_integrated_df['_merge'] == 'both']\n",
    "display(len(audible_goodreads_merged))\n",
    "\n",
    "# display(test_integrated_df)\n",
    "\n",
    "# display(audible_goodreads_merged[~audible_goodreads_merged['isbn'].isna()])\n",
    "display(audible_goodreads_merged.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lists_for_df = []\n",
    "\n",
    "# for index, row in cleaned_audible_df.head(10000).iterrows():\n",
    "for index, row in audible_goodreads_merged.head(100).iterrows():\n",
    "\n",
    "    # if index is > 0 and is a multiple of 100, wait\n",
    "    # this is to deal with the 100 requests per 100 seconds rate limit\n",
    "    # (or at least that is what I found online)\n",
    "    if index > 0 and index % 100 == 0:\n",
    "        time.sleep(40)\n",
    "        print(\"Sleeping, for the rate limit\")\n",
    "\n",
    "    print(\"index:\", index)\n",
    "    # the book title in the audible dataset is in a column called 'name'\n",
    "    \n",
    "    # title = row[\"name\"]\n",
    "    # author = row[\"author\"]\n",
    "    # audible_id = row[\"audible_ID\"]\n",
    "    ISBN = row['isbn']\n",
    "\n",
    "    # book_data = google_books_query(title, author)\n",
    "    book_data = google_books_query(ISBN)\n",
    "\n",
    "\n",
    "    if book_data[\"totalItems\"] == 0:\n",
    "        # print(\"no results for title:\", title, \"with author:\", author)\n",
    "        print(\"no results!!\")\n",
    "    else:\n",
    "        if 'items' in book_data:\n",
    "            # TODO: just use the first????\n",
    "            item = book_data['items'][0]\n",
    "        # for item in book_data[\"items\"]:\n",
    "            # TODO: change this later!!!!!!!!!!\n",
    "            # if we have an exact match on the title\n",
    "\n",
    "            if \"title\" in item[\"volumeInfo\"]:\n",
    "                # if item[\"volumeInfo\"][\"title\"] == title:\n",
    "                    # print(\"exact title match\")\n",
    "                \"\"\"items from google books that I am deciding to keep:\n",
    "                ?? id for the google books ID for this book?\n",
    "\n",
    "                volumeInfo->title, authors, publishedDate, description\n",
    "\n",
    "                volumeInfo->industryIdentifiers (a list of dictionaries)\n",
    "                -> get the one where type is ISBN_10, and ISBN_13, and get the identifier value\n",
    "\n",
    "                volumeInfo->pageCount\n",
    "                volumeInfo->printType\n",
    "                volumeInfo->maturityRating\n",
    "\n",
    "                volumeInfo->previewLink\n",
    "                volumeInfo->infoLink\n",
    "                volumeInfo->canonicalVolumeLink\n",
    "                \"\"\"\n",
    "\n",
    "                authors_string = np.NaN\n",
    "                if \"authors\" in item[\"volumeInfo\"]:\n",
    "                    authors_string = \"\"\n",
    "\n",
    "                    # add all the authors to the same column, separated by a |, and take care of it later\n",
    "                    for author in item[\"volumeInfo\"][\"authors\"]:\n",
    "                        authors_string += author + \"|\"\n",
    "\n",
    "                # default values of NA\n",
    "                ISBN_10 = np.NaN\n",
    "                ISBN_13 = np.NaN\n",
    "\n",
    "                # don't even attempt this loop if this section is not there\n",
    "                if \"industryIdentifier\" in item[\"volumeInfo\"]:\n",
    "                    # loop through the list of dictionaries, get the ISBNs\n",
    "                    for ID in item[\"volumeInfo\"][\"industryIdentifier\"]:\n",
    "                        if ID[\"type\"] == \"ISBN_10\":\n",
    "                            ISBN_10 = ID[\"identifier\"]\n",
    "\n",
    "                        if ID[\"type\"] == \"ISBN_13\":\n",
    "                            ISBN_13 = ID[\"identifier\"]\n",
    "\n",
    "                # had to add this since some books don't have a description\n",
    "                description_string = np.NaN\n",
    "                if \"description\" in item[\"volumeInfo\"]:\n",
    "                    description_string = item[\"volumeInfo\"][\"description\"]\n",
    "\n",
    "                # title\n",
    "                title = np.NaN\n",
    "                if \"title\" in item[\"volumeInfo\"]:\n",
    "                    title = item[\"volumeInfo\"][\"title\"]\n",
    "\n",
    "                # published date\n",
    "                publishedDate = np.NaN\n",
    "                if \"publishedDate\" in item[\"volumeInfo\"]:\n",
    "                    publishedDate = item[\"volumeInfo\"][\"publishedDate\"]\n",
    "\n",
    "                # page count\n",
    "                pagecount = np.NaN\n",
    "                if \"pageCount\" in item[\"volumeInfo\"]:\n",
    "                    pagecount = item[\"volumeInfo\"][\"pageCount\"]\n",
    "\n",
    "                # the order: id, title, authors ****have to still split it up\n",
    "                # published date, description, ISBN 10, ISBN 13, page count, print type\n",
    "                # maturity rating,\n",
    "                # preview link, info link, canonical volume link\n",
    "\n",
    "                list_for_this_book = [\n",
    "                    # the audible ID is here so we can connect the two dataframes later\n",
    "                    # audible_id,\n",
    "                    item[\"id\"],\n",
    "                    item[\"volumeInfo\"][\"title\"],\n",
    "                    authors_string,\n",
    "                    publishedDate,\n",
    "                    description_string,\n",
    "                    ISBN_10,\n",
    "                    ISBN_13,\n",
    "                    pagecount,\n",
    "                    item[\"volumeInfo\"][\"printType\"],\n",
    "                    item[\"volumeInfo\"][\"maturityRating\"],\n",
    "                    item[\"volumeInfo\"][\"previewLink\"],\n",
    "                    item[\"volumeInfo\"][\"infoLink\"],\n",
    "                    item[\"volumeInfo\"][\"canonicalVolumeLink\"],\n",
    "                ]\n",
    "\n",
    "                print(list_for_this_book)\n",
    "\n",
    "                list_of_lists_for_df.append(list_for_this_book)\n",
    "                # break since we already found one perfect match in the google books data set,\n",
    "                # for this entry in the audible data set\n",
    "                break\n",
    "\n",
    "            # else:\n",
    "            #     print(\"no title found! item:\", item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_lists_for_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(google_books_df.head())\n",
    "\n",
    "display(google_books_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clean up the google books DF!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audible_google_integrated_df = cleaned_audible_df.merge(google_books_df, on='audible_ID', indicator=True, how='outer')\n",
    "display(len(test_audible_google_integrated_df[test_audible_google_integrated_df['_merge'] == 'both']))\n",
    "display(len(test_audible_google_integrated_df))\n",
    "display(test_audible_google_integrated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modified code to lookup items in google books by ISBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
