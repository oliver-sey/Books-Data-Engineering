{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books Data Engineering ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spring 2024, by Oliver Seymour\n",
    "\n",
    "\n",
    "#### My Data Sources:\n",
    "\n",
    "Goodreads books and reviews: https://www.kaggle.com/datasets/jealousleopard/goodreadsbooks\n",
    "\n",
    "Audible books and reviews: https://www.kaggle.com/code/satyanarayanam/cleaning-audible-dataset/input\n",
    "\n",
    "***add another here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: uncomment these!!\n",
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# for converting strings to a format that works for URLs\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import my auth secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the .env file to get my API key for Google books\n",
    "# I tried to use the dotenv library but the install didn't work\n",
    "\n",
    "with open('.env', 'r') as file:\n",
    "\tfor line in file:\n",
    "\t\t# we want to skip empty lines or lines that are comments\n",
    "\t\tif line.strip() == '' or line.startswith('#'):\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\t# strip each like and then split it to make a key-value pair\n",
    "\t\tkey, value = line.strip().split('=', 1)\n",
    "\t\t\n",
    "\t\t# set the env variable\n",
    "\t\tos.environ[key] = value\n",
    "\n",
    "# set the API key variable so I can use it later\n",
    "GOOGLE_BOOKS_API_KEY = os.getenv('API_KEY')\n",
    "# set the DB password variable\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Audible dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audible_df = pd.read_csv('Data/audible_copy_pasted.csv', encoding='utf-8')\n",
    "print(\"data from audible:\")\n",
    "display(audible_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check that there are no character encoding issues on a couple rows that had problems before\n",
    "display(audible_df[(audible_df['time'] == '9 hrs and 17 mins') & (audible_df['language'] == 'mandarin_chinese')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Audible dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shape?\n",
    "audible_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many NAs are there?\n",
    "audible_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if the author column always starts with 'Writtenby:'\n",
    "display(audible_df[~audible_df['author'].str.contains(pat=r'^Writtenby:', regex=True)])\n",
    "# wow ok, there are 0 rows that don't have that starting, that will be easy to clean\n",
    "\n",
    "# and if the narrator column always starts with 'Narratedby:'\n",
    "display(audible_df[~audible_df['narrator'].str.contains(pat=r'^Narratedby:', regex=True)])\n",
    "# ok nice, once again they all follow that pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we know these columns always start with 'Writtenby:' and 'Narratedby:', how can we split up the first and last names?\n",
    "# regex: first 'Writtenby: and then either one or more of: (a capital letter followed by one or more lowercase letters), \n",
    "# basically just FirstNameOrMaybeMoreName\n",
    "# or (one or more of: (a capital letter followed by a period), i.e. for an initial, followed by a capital letter and then one or more lowercase\n",
    "# basically A.B.LastNameMaybeMore\n",
    "\n",
    "\n",
    "# display(audible_df[~audible_df[\"author\"].str.contains(pat=r\"^Writtenby:( (([A-Z][a-z]+)+ | (([A-Z][.])+[A-Z][a-z]+))+[,]* )\")])\n",
    "# display(audible_df[audible_df[\"author\"].str.contains(pat=r\"^Writtenby:((([A-Z][.])+[A-Z][a-z]+)+[,]*)\")])\n",
    "display(audible_df[~audible_df[\"author\"].str.contains(pat=r\"^Writtenby:([A-Z][a-z]+)+\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check for inconsistencies in the formatting of the 'time' column\n",
    "# do they all have 'and'?\n",
    "display(audible_df[~audible_df['time'].str.contains(pat=r'and', regex=True)].head())\n",
    "# ok not all do\n",
    "\n",
    "# do they all follow the <numbers><space><letters> etc. pattern?\n",
    "display(audible_df[~audible_df['time'].str.contains(pat=r'^\\d+\\s+[A-Za-z]+')].head())\n",
    "\n",
    "# let's get the unique values:\n",
    "audible_df.loc[~audible_df['time'].str.contains(r'^\\d+\\s+[A-Za-z]+', regex=True), 'time'].unique()\n",
    "# ok seems like it always contains 'and', except if the value is 'Less than 1 minute'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the stars column\n",
    "# let's see what values there are:\n",
    "audible_df['stars'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How often is the stars column 'Not rated yet', or some other string that doesn't contain a digit?\n",
    "display(audible_df[~audible_df['stars'].str.contains(r'\\d')].shape)\n",
    "# wow that's a lot of books that aren't rated yet!\n",
    "display(audible_df[~audible_df['stars'].str.contains(r'\\d')])\n",
    "\n",
    "# what unique values are there?\n",
    "audible_df[~audible_df['stars'].str.contains(r'\\d')]['stars'].unique()\n",
    "# ok seems to just be 'Not rated yet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the values in 'stars' all follow the same pattern?\n",
    "print(\"check that it's always out of 5 stars:\")\n",
    "display(audible_df[~(audible_df['stars'].str.contains(r'out of 5 stars') \n",
    "\t\t\t\t\t | audible_df['stars'].str.contains(r'Not rated yet'))]['stars'].unique())\n",
    "\n",
    "print(\"check that its always either an integer number of stars, or 1 decimal digit, or Not rated yet:\")\n",
    "# at the start of the string, one digit, then an optional period, then an optional digit, then it has to be a space\n",
    "display(audible_df[~(audible_df['stars'].str.contains(r'^\\d[.]*\\d*\\s') \n",
    "\t\t\t\t\t | audible_df['stars'].str.contains(r'Not rated yet'))]['stars'].unique())\n",
    "\n",
    "print(\"values in 'stars' where they don't follow the 'X.X out of 5 starsXXX (1-3 digits) rating[maybe an 's']' or 'Not rated yet' pattern:\")\n",
    "display(audible_df[~(audible_df['stars'].str.contains(r'\\d[.]*\\d* out of 5 stars\\d{1,3} rating[s]*') \n",
    "\t\t\t\t\t | audible_df['stars'].str.contains(r'Not rated yet'))]['stars'].unique())\n",
    "\n",
    "print(\"also excluding e.g. '4.5 out of 5 stars1,869 ratings', with the comma in the ratings count:\")\n",
    "display(audible_df[~(audible_df['stars'].str.contains(r'\\d[.]*\\d* out of \\d stars\\d{1,3} rating[s]*') \n",
    "\t\t\t\t\t | audible_df['stars'].str.contains(r'Not rated yet')\n",
    "\t\t\t\t\t | audible_df['stars'].str.contains(r'\\d[.]*\\d* out of \\d stars\\d+[,]\\d+ rating[s]*')\n",
    "\t\t\t\t\t )]['stars'].unique())\n",
    "\n",
    "# ok it seems like we only have a few possible patterns in this column\n",
    "# and the stars are always out of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the release date column always follow the pattern <numbers> - <numbers> - <numbers>?\n",
    "display(audible_df[~audible_df['releasedate'].str.contains(r'^\\d+-\\d+-\\d+', regex=True)])\n",
    "# Ok nice they all follow that pattern\n",
    "\n",
    "display(audible_df[~audible_df['releasedate'].str.contains(r'^\\d{2}-\\d{2}-\\d{4}', regex=True)])\n",
    "# but it appears they don't always follow DD-MM-YYYY, sometimes it is D-M-YYYY, and sometimes it is just YY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what values are there in language?\n",
    "display(audible_df['language'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Audible dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a deep copy\n",
    "cleaned_audible_df = audible_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the price column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's clean up the price column and get it into a float\n",
    "# display(cleaned_audible_df[cleaned_audible_df['cleaned_price'].str.contains(',')])\n",
    "\n",
    "# since there are commas in the numbers, e.g. 1,234.00, we will have to get rid of commas\n",
    "cleaned_audible_df['price_no_commas'] = cleaned_audible_df['price'].str.replace(',', '')\n",
    "\n",
    "# check if the comma removing worked correctly:\n",
    "# .option_context(\"display.min_rows\", 10000):\n",
    "print('rows where price contains a comma:')\n",
    "display(cleaned_audible_df[cleaned_audible_df['price'].str.contains(',')])\n",
    "\n",
    "print(\"Check the NA's:\")\n",
    "display(cleaned_audible_df.isna().sum())\n",
    "# display(cleaned_audible_df[cleaned_audible_df['price_no_commas'].isna()])\n",
    "\n",
    "print(\"rows where price has a non-zero digit after the decimal place:\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['price_no_commas'].str.contains(r'[.][1-9]', regex=True, na=False)])\n",
    "# ok there are some prices that have a fraction, so we can't just make them ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what unique values are in the price column besides just digits\n",
    "cleaned_audible_df[cleaned_audible_df['price'].str.contains(r'[A-Za-z]', regex=True)]['price'].unique()\n",
    "# Ok nice only 'Free', let's make those 0\n",
    "\n",
    "cleaned_audible_df['cleaned_price'] = cleaned_audible_df['price_no_commas'].apply(lambda value: 0 if value == 'Free' else value)\n",
    "\n",
    "# let's also convert to a float\n",
    "cleaned_audible_df['cleaned_price'] = cleaned_audible_df['cleaned_price'].astype(float)\n",
    "\n",
    "display(cleaned_audible_df.head())\n",
    "\n",
    "display(cleaned_audible_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert price from rupees to dollars\n",
    "# the data was scraped from Audible on 4/7/22, and I was able to get a conversion rate of \n",
    "# 1 rupee = 0.013 USD from the next day\n",
    "# and then with inflation to the latest data (January 2024), is 0.01387 USD\n",
    "\n",
    "# also round to 2 decimal places to get only to the level of cents\n",
    "cleaned_audible_df['cleaned_price_USD'] = round(cleaned_audible_df['cleaned_price'] * 0.01387, 2)\n",
    "\n",
    "cleaned_audible_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "# with pd.option_context(\"display.min_rows\", 10000):\n",
    "display(cleaned_audible_df)\n",
    "\n",
    "# looks good, now we can drop the other columns and rename cleaned_price\n",
    "cleaned_audible_df.drop(columns=['price', 'price_no_commas', 'cleaned_price'], inplace=True)\n",
    "# cleaned_audible_df.rename(columns= {'cleaned_price' : 'cleaned_price_rupees'}, inplace=True)\n",
    "\n",
    "print(\"After dropping and renaming:\")\n",
    "display(cleaned_audible_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the author and narrator columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my exploration I found that every row in the author and narrator column started with\n",
    "# 'Writtenby:' and 'Narratedby:', so let's remove that\n",
    "\n",
    "# replace 'Writtenby:' with an empty string\n",
    "cleaned_audible_df['author'] = cleaned_audible_df['author'].str.replace(pat=r'^Writtenby:', repl='', regex=True)\n",
    "\n",
    "# replace 'Narrattedby:' with an empty string\n",
    "cleaned_audible_df['narrator'] = cleaned_audible_df['narrator'].str.replace(pat=r'^Narratedby:', repl='', regex=True)\n",
    "\n",
    "display(cleaned_audible_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the releasedate column (converting to timestamp):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the release date to a datetime object\n",
    "# **Important note, they seem to be in a DD-MM-YYYY (or sometimes D-M-YY, etc. but day first) format, \n",
    "# since there are some dates like 30-10-18, 25-11-14 (so the day must be first), but there is also 1-5-2018\n",
    "cleaned_audible_df['cleaned_releasedate'] = pd.to_datetime(cleaned_audible_df['releasedate'], dayfirst=True, format='mixed')\n",
    "\n",
    "# temporarily display a lot of rows to check it worked\n",
    "with pd.option_context(\"display.min_rows\", 10):\n",
    "\t# look at just rows where releasedate had a weird format to especially make sure it worked for those\n",
    "\tdisplay(cleaned_audible_df[~cleaned_audible_df['releasedate'].str.contains(r'\\d{2}-\\d{2}-\\d{4}', regex=True)])\n",
    "\tdisplay(cleaned_audible_df)\n",
    "\n",
    "# ok seems to have worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure there are no weird values in cleaned_releasedate\n",
    "print(\"min release date values:\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['cleaned_releasedate'] == cleaned_audible_df['cleaned_releasedate'].min()])\n",
    "# ok the min values look reasonable\n",
    "\n",
    "# check books that were released after today's date\n",
    "print(\"release date values that are after today's date:\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['cleaned_releasedate'] > pd.to_datetime('today').normalize()])\n",
    "# I don't think this is a mistake since one of the original releasedate values is '9-8-2024'\n",
    "# These have very weird character encoding issues and I don't think it makes sense to have books that aren't release yet\n",
    "# so I'm going to drop these\n",
    "\n",
    "cleaned_audible_df = cleaned_audible_df[cleaned_audible_df['cleaned_releasedate'] <= pd.to_datetime('today').normalize()]\n",
    "print(\"new max release date value:\")\n",
    "display(cleaned_audible_df['cleaned_releasedate'].max())\n",
    "# ok nice, now the max value was released about 2 weeks ago, that seems reasonable\n",
    "\n",
    "# now let's drop the releasedate column\n",
    "cleaned_audible_df.drop(columns=['releasedate'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the time column and extracting hours and mins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt at converting the time column by splitting it into hours, mins, etc. components in different columns and then combining again\n",
    "print(\"df with the hours extracted:\")\n",
    "display(cleaned_audible_df['time'].str.extract(pat=r'([A-Za-z]+\\s*)+')[0].unique())\n",
    "\n",
    "# at the start of the string, get one or more digits, then a white space, hr, maybe an s\n",
    "# but only keep the digits\n",
    "# i.e. we get '8 hrs' or '1 hr' and we only keep the 8 or 1\n",
    "cleaned_audible_df['hours'] = cleaned_audible_df['time'].str.extract(pat=r'^(\\d+)\\shr[s]*')\n",
    "# seems to have worked\n",
    "# we have an NA if it is 0 hours, so let's fill the NAs with 0's\n",
    "cleaned_audible_df['hours'] = cleaned_audible_df['hours'].fillna(0)\n",
    "\n",
    "# with pd.option_context(\"display.min_rows\", 10000):\n",
    "display(cleaned_audible_df)\n",
    "\n",
    "# check that it worked for when it is just '1 hr', not hrs\n",
    "print(\"rows with time == '1 hr':\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['time'] == '1 hr'])\n",
    "\n",
    "# check that the fillNA worked correctly\n",
    "print(\"rows with 0 in the hours column:\")\n",
    "# with pd.option_context(\"display.min_rows\", 10000):\n",
    "display(cleaned_audible_df[cleaned_audible_df['hours'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the minutes\n",
    "# cleaned_audible_df['minutes'] = cleaned_audible_df['time'].str.extract(r'')\n",
    "cleaned_audible_df[cleaned_audible_df['time'].str.contains(r'min$', regex=True)]\n",
    "# ok it seems we have to be careful with not getting 'Less than 1 minute' and something like '1 hr and 1 min' mixed up\n",
    "\n",
    "# one or more digits, then 1 whitespace, then 'min', then maybe an s, all this at the end of the string\n",
    "# only capture the digits though\n",
    "# it is important that it's at the end of the string so we don't accidentally capture the 1 from 'Less than 1 minute', \n",
    "# we only want the number when the string ends in min or mins\n",
    "print(\"df with minutes extracted:\")\n",
    "cleaned_audible_df['minutes'] = cleaned_audible_df['time'].str.extract(r'(\\d+)\\smin[s]*$')\n",
    "# cleaned_audible_df[cleaned_audible_df['time'].str.contains(r'(\\d+\\smin[s]*$)')]\n",
    "\n",
    "# with pd.option_context(\"display.min_rows\", 10000):\n",
    "display(cleaned_audible_df)\n",
    "\n",
    "# check that it correctly did NA for rows where there are no minutes\n",
    "print(\"rows with NA in the minutes column:\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['minutes'].isna()])\n",
    "\n",
    "# the minutes column seems to have worked nicely, let's fill in the NAs with 0's\n",
    "cleaned_audible_df['minutes'] = cleaned_audible_df['minutes'].fillna(0)\n",
    "\n",
    "print(\"cleaned_audible_df with minutes NAs filled in with 0's:\")\n",
    "# with pd.option_context(\"display.min_rows\", 10000):\n",
    "display(cleaned_audible_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the columns to integer\n",
    "cleaned_audible_df['hours'] = cleaned_audible_df['hours'].astype(int)\n",
    "cleaned_audible_df['minutes'] = cleaned_audible_df['minutes'].astype(int)\n",
    "\n",
    "cleaned_audible_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many rows are only 1 minute, to help us figure out what to do with the 'Less than 1 minute'\n",
    "cleaned_audible_df[(cleaned_audible_df['hours'] == 0) & (cleaned_audible_df['minutes'] == 1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what to do with rows where it is 'Less than 1 minute'????\n",
    "# let's see what unique values there are in the time column where we got NA for both hours and mins, and filled them both in with 0's:\n",
    "display(cleaned_audible_df[(cleaned_audible_df['hours'] == 0) & (cleaned_audible_df['minutes'] == 0)]['time'].unique())\n",
    "# Ok, just 'Less than 1 minute'. I will have to figure out how to take care of this\n",
    "\n",
    "# how many are there?\n",
    "print(\"# of rows where time is 'Less than 1 minute':\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['time'] == 'Less than 1 minute'].shape[0])\n",
    "\n",
    "# ok since it is just 61 in 88k, and since we don't have any precision smaller than minutes, \n",
    "# I think we should just round this to 1 minute\n",
    "cleaned_audible_df.loc[cleaned_audible_df['time'] == 'Less than 1 minute', 'minutes'] = 1\n",
    "\n",
    "print(\"rows where 'time' is 'Less than 1 minute':\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['time'] == 'Less than 1 minute'].head())\n",
    "\n",
    "# ok looks good, now that we have a nice hours and minutes column, we can convert that to a timedelta and drop 'time'\n",
    "# cleaned_audible_df.drop(columns=['time'], inplace=True)\n",
    "\n",
    "# convert the hours and minutes columns to separate columns that are a pandas time delta\n",
    "cleaned_audible_df['hours_timedelta'] = pd.to_timedelta(cleaned_audible_df['hours'], unit='h')\n",
    "cleaned_audible_df['minutes_timedelta'] = pd.to_timedelta(cleaned_audible_df['minutes'], unit='m')\n",
    "\n",
    "# add the two timedelta columns together to get the full runtime\n",
    "cleaned_audible_df['cleaned_runtime'] = cleaned_audible_df['hours_timedelta'] + cleaned_audible_df['minutes_timedelta']\n",
    "\n",
    "print(\"With timedelta columns:\")\n",
    "display(cleaned_audible_df)\n",
    "\n",
    "# drop the unnecessary columns, including time\n",
    "cleaned_audible_df.drop(columns=['time', 'hours', 'minutes', 'hours_timedelta', 'minutes_timedelta'], inplace=True)\n",
    "print(\"With columns dropped:\")\n",
    "display(cleaned_audible_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the stars column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok we know there are only a few patterns that values in this column follow\n",
    "# at the start of the string, get a number and then optionally a decimal and another number\n",
    "# (we know from the exploration that it's only ever an integer for the stars, or one decimal place)\n",
    "cleaned_audible_df['star_count'] = cleaned_audible_df['stars'].str.extract(r'^(\\d[.]*\\d*)')\n",
    "\n",
    "cleaned_audible_df\n",
    "# ok we have NAs in rows with no ratings yet but I think that actually makes sense as a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_audible_df['ratings_count'] = cleaned_audible_df['stars'].str.extract(r'([\\d,]+) rating')\n",
    "\n",
    "display(cleaned_audible_df.isna().sum())\n",
    "# check that it worked\n",
    "print(\"df after making ratings_count:\")\n",
    "display(cleaned_audible_df.head())\n",
    "\n",
    "# check that it worked when there is a comma in the ratings number (e.g. 1,400 ratings)\n",
    "print(\"rows with a comma in stars:\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['stars'].str.contains(',')].head())\n",
    "\n",
    "\n",
    "# for when there are no ratings, I think it makes sense to have ratings_count be 0, and not NA\n",
    "# but star_count should be NA I think since 0 stars means something different from no reviews\n",
    "cleaned_audible_df['ratings_count'] = cleaned_audible_df['ratings_count'].fillna(0)\n",
    "\n",
    "# check that it worked when there are no ratings\n",
    "print(\"rows where stars is 'Not rated yet':\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['stars'].str.contains('Not rated yet')].head())\n",
    "\n",
    "# ok looks good, now let's remove commas from the ratings_count\n",
    "cleaned_audible_df['ratings_count'] = cleaned_audible_df['ratings_count'].str.replace(',', '')\n",
    "# another fillna?? since I was getting weird behavior\n",
    "cleaned_audible_df['ratings_count'] = cleaned_audible_df['ratings_count'].fillna(0)\n",
    "\n",
    "print(\"After removing commas from ratings_count:\")\n",
    "display(cleaned_audible_df.head())\n",
    "print(\"check rows with commas one last time:\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['stars'].str.contains(',')].head())\n",
    "# display(cleaned_audible_df[cleaned_audible_df['ratings_count'].str.contains('^0', regex=True, na=True)].head())\n",
    "\n",
    "print(\"check NAs:\")\n",
    "display(cleaned_audible_df[cleaned_audible_df['ratings_count'].isna()].head())\n",
    "\n",
    "display(cleaned_audible_df.isna().sum())\n",
    "\n",
    "# ok looks good, now we can drop the stars column\n",
    "cleaned_audible_df.drop(columns=['stars'], inplace=True)\n",
    "\n",
    "# convert the star_count to float, and ratings_count to int\n",
    "cleaned_audible_df['star_count'] = cleaned_audible_df['star_count'].astype(float)\n",
    "cleaned_audible_df['ratings_count'] = cleaned_audible_df['ratings_count'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a unique ID in each row of the Audible DF\n",
    "# Even though I know we will need to drop some rows after this, I think this is a good time to do this\n",
    "\n",
    "# I think name (title of the book) is a good column to use for this\n",
    "# first just as a check, make sure we have only unique values in 'name'\n",
    "\n",
    "# wow ok we do have duplicates by book name\n",
    "# there are even duplicates when you group together the name, author, and narrator\n",
    "# and 179 duplicates when you group together the name, author, narrator, language, and the release date\n",
    "\t# I think this last combination doesn't make sense for there to be duplicates\n",
    "\t# even if there are different runtimes of the audiobooks, I don't think it makes sense\n",
    "\t# to have these duplicates, and it is not a lot of rows\n",
    "\n",
    "# duplicates = cleaned_audible_df.duplicated(subset=['name', 'author', 'narrator', 'language', 'cleaned_releasedate'])\n",
    "# display(cleaned_audible_df[duplicates])\n",
    "\n",
    "display(len(cleaned_audible_df))\n",
    "display(len(cleaned_audible_df.drop_duplicates(subset=['name', 'author', 'narrator', 'language', 'cleaned_releasedate'])))\n",
    "# ok looks like this will only remove the 179 rows, like expected\n",
    "# let's drop them\n",
    "cleaned_audible_df.drop_duplicates(subset=['name', 'author', 'narrator', 'language', 'cleaned_releasedate'], inplace=True)\n",
    "\n",
    "# now that we have done that dropping, I feel like each row is ok to get assigned a unique ID\n",
    "# since each row should be unique at this point, and I would either need to use 1 unique column\n",
    "# for pd.factorize(), which we don't really have, or do something complicated to combine multiple\n",
    "# columns to make a unique combination, I am just going to use each row's index\n",
    "cleaned_audible_df['audible_ID'] = cleaned_audible_df.index\n",
    "\n",
    "cleaned_audible_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A final check of the cleaned dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"audible:\")\n",
    "display(cleaned_audible_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from Google Books API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version for name and author\n",
    "books_found_not_perfect_match = 0\n",
    "books_found_perfect_match = 0\n",
    "books_not_found = 0\n",
    "\n",
    "\n",
    "def google_books_query(title, author):\n",
    "\t# use the global version of these variables\n",
    "\tglobal books_found_not_perfect_match\n",
    "\tglobal books_found_perfect_match\n",
    "\tglobal books_not_found\n",
    "\n",
    "\tconverted_title = convert_string_for_url(title)\n",
    "\tconverted_author = convert_string_for_url(author)\n",
    "\n",
    "\turl = (\n",
    "\t\t# TODO: should I do exact phrase or not??\n",
    "\t\t \"https://www.googleapis.com/books/v1/volumes?q=\\\"exact%20phrase\\\"intitle:\"\n",
    "\t\t#  \"https://www.googleapis.com/books/v1/volumes?q=intitle:\"\n",
    "\t\t + converted_title\n",
    "\t\t + \"inauthor:\"\n",
    "\t\t + converted_author\n",
    "\t\t + \"&key=\"\n",
    "\t\t+ GOOGLE_BOOKS_API_KEY\n",
    "\t)\n",
    "\n",
    "\t# send a request and get a JSON response\n",
    "\tresp = urlopen(url)\n",
    "\n",
    "\t# parse JSON into Python as a dictionary\n",
    "\tbook_data = json.load(resp)\n",
    "\t\n",
    "\ttry:\n",
    "\t\tif \"items\" in book_data:\n",
    "\t\t\t\n",
    "\t\t\tvolume_info = book_data[\"items\"][0][\"volumeInfo\"]\n",
    "\t\t\tauthor = volume_info[\"authors\"]\n",
    "\t\t\tprettify_author = author if len(author) > 1 else author[0]\n",
    "\n",
    "\t\t\tprint(\"query title:\", title)\n",
    "\t\t\tprint(\"found title:\", volume_info['title'])\n",
    "\n",
    "\t\t\tprint(\"query author:\", author)\n",
    "\t\t\tprint(\"found author:\", prettify_author)\n",
    "\n",
    "\t\t\t# perfect match\n",
    "\t\t\tif title == volume_info[\"title\"]:\n",
    "\t\t\t\t books_found_perfect_match += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\t books_found_not_perfect_match += 1\n",
    "\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tprint(\"******no results found for title:\", title, \"author:\", author)\n",
    "\t\t\tbooks_not_found += 1\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"***\\n\\n\\nError:\", e)\n",
    "\n",
    "\treturn book_data\n",
    "\n",
    "\n",
    "def convert_string_for_url(input_string):\n",
    "\n",
    "\t# Encode the string for URL\n",
    "\tencoded_string = quote(input_string)\n",
    "\n",
    "\treturn encoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"books_found_not_perfect_match:\", books_found_not_perfect_match)\n",
    "# print(\"books_found_perfect_match:\", books_found_perfect_match)\n",
    "# print(\"books_not_found:\", books_not_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Google Books dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_lists_for_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clean this up!!\n",
    "\n",
    "# TODO: make this loop over all!!\n",
    "for index in range(3000):\n",
    "    print(\"row num:\", index)\n",
    "    row = cleaned_audible_df.iloc[index]\n",
    "# for index, row in cleaned_audible_df.iterrows():\n",
    "# for index, row in cleaned_audible_df.head(10000).iterrows():\n",
    "# for index, row in audible_goodreads_merged.head(100).iterrows():\n",
    "\n",
    "    # if index is > 0 and is a multiple of 100, wait\n",
    "    # this is to deal with the 100 requests per 100 seconds rate limit\n",
    "    # (or at least that is what I found online)\n",
    "    if index > 0 and index % 100 == 0:\n",
    "        time.sleep(60)\n",
    "        print(\"Sleeping, for the rate limit\")\n",
    "\n",
    "    print(\"index:\", index)\n",
    "    # the book title in the audible dataset is in a column called 'name'\n",
    "    \n",
    "    title = row[\"name\"]\n",
    "    author = row[\"author\"]\n",
    "    audible_id = row[\"audible_ID\"]\n",
    "    # ISBN = row['isbn']\n",
    "\n",
    "    book_data = google_books_query(title, author)\n",
    "    # book_data = google_books_query(ISBN)\n",
    "\n",
    "\n",
    "    if book_data[\"totalItems\"] == 0:\n",
    "        # print(\"no results for title:\", title, \"with author:\", author)\n",
    "        print(\"no results!!\")\n",
    "    else:\n",
    "        if 'items' in book_data:\n",
    "            # TODO: just use the first????\n",
    "            item = book_data['items'][0]\n",
    "            for item in book_data[\"items\"]:\n",
    "                # TODO: change this later!!!!!!!!!!\n",
    "                # if we have an exact match on the title\n",
    "\n",
    "                if \"title\" in item[\"volumeInfo\"]:\n",
    "                    # if item[\"volumeInfo\"][\"title\"] == title:\n",
    "                        # print(\"exact title match\")\n",
    "                    \"\"\"items from google books that I am deciding to keep:\n",
    "                    ?? id for the google books ID for this book?\n",
    "\n",
    "                    volumeInfo->title, authors, publishedDate, description\n",
    "\n",
    "                    volumeInfo->industryIdentifiers (a list of dictionaries)\n",
    "                    -> get the one where type is ISBN_10, and ISBN_13, and get the identifier value\n",
    "\n",
    "                    volumeInfo->pageCount\n",
    "                    volumeInfo->printType\n",
    "                    volumeInfo->maturityRating\n",
    "\n",
    "                    volumeInfo->previewLink\n",
    "                    volumeInfo->infoLink\n",
    "                    volumeInfo->canonicalVolumeLink\n",
    "                    \"\"\"\n",
    "\n",
    "                    authors_string = np.NaN\n",
    "                    if \"authors\" in item[\"volumeInfo\"]:\n",
    "                        authors_string = \"\"\n",
    "\n",
    "                        # add all the authors to the same column, separated by a |, and take care of it later\n",
    "                        for author in item[\"volumeInfo\"][\"authors\"]:\n",
    "                            authors_string += author + \"|\"\n",
    "\n",
    "                    # default values of NA\n",
    "                    ISBN_10 = np.NaN\n",
    "                    ISBN_13 = np.NaN\n",
    "\n",
    "                    # don't even attempt this loop if this section is not there\n",
    "                    if \"industryIdentifier\" in item[\"volumeInfo\"]:\n",
    "                        # loop through the list of dictionaries, get the ISBNs\n",
    "                        for ID in item[\"volumeInfo\"][\"industryIdentifier\"]:\n",
    "                            if ID[\"type\"] == \"ISBN_10\":\n",
    "                                ISBN_10 = ID[\"identifier\"]\n",
    "\n",
    "                            if ID[\"type\"] == \"ISBN_13\":\n",
    "                                ISBN_13 = ID[\"identifier\"]\n",
    "\n",
    "                    # had to add this since some books don't have a description\n",
    "                    description_string = np.NaN\n",
    "                    if \"description\" in item[\"volumeInfo\"]:\n",
    "                        description_string = item[\"volumeInfo\"][\"description\"]\n",
    "\n",
    "                    # title\n",
    "                    title = np.NaN\n",
    "                    if \"title\" in item[\"volumeInfo\"]:\n",
    "                        title = item[\"volumeInfo\"][\"title\"]\n",
    "\n",
    "                    # published date\n",
    "                    publishedDate = np.NaN\n",
    "                    if \"publishedDate\" in item[\"volumeInfo\"]:\n",
    "                        publishedDate = item[\"volumeInfo\"][\"publishedDate\"]\n",
    "\n",
    "                    # page count\n",
    "                    pagecount = np.NaN\n",
    "                    if \"pageCount\" in item[\"volumeInfo\"]:\n",
    "                        pagecount = item[\"volumeInfo\"][\"pageCount\"]\n",
    "\n",
    "                    # the order: id, title, authors ****have to still split it up\n",
    "                    # published date, description, ISBN 10, ISBN 13, page count, print type\n",
    "                    # maturity rating,\n",
    "                    # preview link, info link, canonical volume link\n",
    "\n",
    "                    list_for_this_book = [\n",
    "                        # the audible ID is here so we can connect the two dataframes later\n",
    "                        audible_id,\n",
    "                        title,\n",
    "                        item[\"id\"],\n",
    "                        item[\"volumeInfo\"][\"title\"],\n",
    "                        authors_string,\n",
    "                        publishedDate,\n",
    "                        description_string,\n",
    "                        ISBN_10,\n",
    "                        ISBN_13,\n",
    "                        pagecount,\n",
    "                        item[\"volumeInfo\"][\"printType\"],\n",
    "                        item[\"volumeInfo\"][\"maturityRating\"],\n",
    "                        item[\"volumeInfo\"][\"previewLink\"],\n",
    "                        item[\"volumeInfo\"][\"infoLink\"],\n",
    "                        item[\"volumeInfo\"][\"canonicalVolumeLink\"],\n",
    "                    ]\n",
    "\n",
    "                    print(list_for_this_book)\n",
    "\n",
    "                    list_of_lists_for_df.append(list_for_this_book)\n",
    "                    # break since we already found one perfect match in the google books data set,\n",
    "                    # for this entry in the audible data set\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    print(\"no title found! item:\", item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"books_found_not_perfect_match:\", books_found_not_perfect_match)\n",
    "print(\"books_found_perfect_match:\", books_found_perfect_match)\n",
    "print(\"books_not_found:\", books_not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_lists_for_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_books_df = pd.DataFrame(\n",
    "    list_of_lists_for_df,\n",
    "    columns=[\n",
    "\t\t\"audible_ID\",\n",
    "        \"audible_title\",\n",
    "        \"Google_books_ID\",\n",
    "        \"title\",\n",
    "        \"authors_string\",\n",
    "        \"publishedDate\",\n",
    "        \"description_string\",\n",
    "        \"ISBN_10\",\n",
    "        \"ISBN_13\",\n",
    "        \"pagecount\",\n",
    "        \"printType\",\n",
    "        \"maturityRating\",\n",
    "        \"previewLink\",\n",
    "        \"infoLink\",\n",
    "        \"canonicalVolumeLink\",\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Google books df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(google_books_df.shape)\n",
    "\n",
    "display(google_books_df.head())\n",
    "\n",
    "display(google_books_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clean up the google books DF!!!!!!!\n",
    "\n",
    "# TODO: should we drop these?? what good is a book with no author?\n",
    "# I think I'm going to keep it since I have such little data at the moment\n",
    "google_books_df[google_books_df['authors_string'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Google books df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_google_books_df = google_books_df.copy()\n",
    "\n",
    "\n",
    "# it seems something went wrong with getting the ISBNs, all the values are NA\n",
    "# I am going to drop these columns\n",
    "# TODO: *******uncomment this!!!!\n",
    "# cleaned_google_books_df = cleaned_google_books_df.drop(columns=['ISBN_10', 'ISBN_13'])\n",
    "\n",
    "\n",
    "cleaned_google_books_df['authors_string'].unique()\n",
    "\n",
    "display(cleaned_google_books_df[cleaned_google_books_df['authors_string'].isna()])\n",
    "# not sure why we have NAs in the author, but I'm going to leave it since the rest of the columns seem mostly fine and I need the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the publishedDate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are definitely NAs in this column\n",
    "cleaned_google_books_df[(~pd.isnull(cleaned_google_books_df['publishedDate'])) & (~cleaned_google_books_df['publishedDate'].str.contains(r'\\d{4}-\\d{2}-\\d{2}', na=False, regex=True))]\n",
    "# wow ok lots of different values, even just a year\n",
    "# I am not going to clean this up here, since in the last project I had to put things back into strings to load it into the DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the pageCount column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cleaned_google_books_df.isna().sum())\n",
    "# ok some NAs here\n",
    "display(cleaned_google_books_df['pagecount'].unique())\n",
    "# pretty reasonable values though. Let's convert to int\n",
    "\n",
    "# fill NAs with -1\n",
    "cleaned_google_books_df['pagecount'] = cleaned_google_books_df['pagecount'].fillna(-1)\n",
    "# convert to int\n",
    "cleaned_google_books_df['pagecount'] = cleaned_google_books_df['pagecount'].astype(int)\n",
    "# replace -1 with NAs again, since I think it makes more sense to have it as NA than a specific number\n",
    "# value, since we don't know what number it is\n",
    "cleaned_google_books_df['pagecount'] = cleaned_google_books_df['pagecount'].replace(-1, np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audible_google_integrated_df = cleaned_audible_df.merge(google_books_df, on='audible_ID', indicator=True, how='outer')\n",
    "audible_google_integrated_df = cleaned_audible_df.merge(cleaned_google_books_df, on='audible_ID', how='right')\n",
    "# display(len(audible_google_integrated_df[audible_google_integrated_df['_merge'] == 'both']))\n",
    "display(len(audible_google_integrated_df))\n",
    "display(audible_google_integrated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_audible_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make tables for links, ratings, and books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframes for the other tables in the RDB\n",
    "links_df = google_books_df[\n",
    "    [\"Google_books_ID\", \"previewLink\", \"infoLink\", \"canonicalVolumeLink\"]\n",
    "]\n",
    "\n",
    "audible_ratings_df = audible_google_integrated_df[\n",
    "    [\"Google_books_ID\", \"star_count\", \"ratings_count\"]\n",
    "]\n",
    "\n",
    "\n",
    "books_df = audible_google_integrated_df[\n",
    "    [\n",
    "        \"Google_books_ID\",\n",
    "        \"title\",\n",
    "        \"description_string\",\n",
    "        \"printType\",\n",
    "        \"maturityRating\",\n",
    "        \"cleaned_runtime\",\n",
    "        \"pagecount\",\n",
    "        \"cleaned_price_USD\",\n",
    "        \"publishedDate\",\n",
    "        \"cleaned_releasedate\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# rename columns just to clean it up\n",
    "books_df = books_df.rename(\n",
    "    columns=[\n",
    "        {\"description_string\": \"description\"},\n",
    "        {\"printType\": \"print_type\"},\n",
    "        {\"maturityRating\": \"maturity_rating\"},\n",
    "        {\"cleaned_runtime\": \"audiobook_runtime\"},\n",
    "        {\"pagecount\": \"num_pages\"},\n",
    "        {\"cleaned_price_USD\": \"audiobook_price_USD\"},\n",
    "        {\"publishedDate\": \"book_published_date\"},\n",
    "        {\"cleaned_releasedate\": \"audible_release_date\"},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make tables for authors and narrators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table of Google_books_ID's and author names\n",
    "\n",
    "book_IDs_and_authors = []\n",
    "book_IDs_and_narrators = []\n",
    "\n",
    "\n",
    "# # split up the authors_string column on the | that I added, and strip off whitespace\n",
    "for index, row in audible_google_integrated_df.head(1000).iterrows():\n",
    "\t# deal with the authors\n",
    "\tif not pd.isnull(row['authors_string']):\n",
    "\t\t# print(\"authors_string:\", row['authors_string'])\n",
    "\t\tauthors_for_this_book = row['authors_string'].split('|')\n",
    "\t\t# print(authors_for_this_book)\n",
    "\n",
    "\t\tbook_ID = row['Google_books_ID']\n",
    "\n",
    "\t\t# loop through authors on this book, but ignore if it's an empty string from the splitting\n",
    "\t\tfor author in authors_for_this_book:\n",
    "\t\t\tif author.strip() != '':\n",
    "\t\t\t\t# add the pair to the list that will get turned into a df\n",
    "\t\t\t\tbook_IDs_and_authors.append([book_ID, author.strip()])\n",
    "\n",
    "\n",
    "\t# deal with the narrators\n",
    "\tif not pd.isnull(row['narrator']):\n",
    "\t\t# print(\"narrator:\", row['narrator'])\n",
    "\t\tnarrators_for_this_book = row['narrator'].split(',')\n",
    "\t\t# print(narrators_for_this_book)\n",
    "\n",
    "\t\tbook_ID = row['Google_books_ID']\n",
    "\n",
    "\t\t# loop through narrators on this book, but ignore if it's an empty string from the splitting\n",
    "\t\tfor narrator in narrators_for_this_book:\n",
    "\t\t\tif narrator.strip() != '':\n",
    "\t\t\t\t# add the pair to the list that will get turned into a df\n",
    "\t\t\t\tbook_IDs_and_narrators.append([book_ID, narrator.strip()])\n",
    "\t\n",
    "\n",
    "# display(book_IDs_and_authors)\n",
    "book_IDs_and_authors_df = pd.DataFrame(book_IDs_and_authors, columns=['Google_books_ID', 'author_name'])\n",
    "\n",
    "print(\"duplicated rows in book_IDs_and_authors_df:\")\n",
    "display(book_IDs_and_authors_df[book_IDs_and_authors_df.duplicated()])\n",
    "# we have 5 duplicate combinations of book ID and author name, which I don't really understand, but I'm going to drop it\n",
    "book_IDs_and_authors_df = book_IDs_and_authors_df.drop_duplicates()\n",
    "\n",
    "print(\"book_IDs_and_authors_df without duplicates:\")\n",
    "display(book_IDs_and_authors_df)\n",
    "\n",
    "\n",
    "# narrators:\n",
    "book_IDs_and_narrators_df = pd.DataFrame(book_IDs_and_narrators, columns=['Google_books_ID', 'narrator_name'])\n",
    "\n",
    "print(\"duplicated rows in book_IDs_and_narrators_df:\")\n",
    "display(book_IDs_and_narrators_df[book_IDs_and_narrators_df.duplicated()])\n",
    "\n",
    "# I also don't understand the duplicates here, but I am going to drop them\n",
    "book_IDs_and_narrators_df = book_IDs_and_narrators_df.drop_duplicates()\n",
    "\n",
    "print(\"book_IDs_and_narrators_df without duplicates:\")\n",
    "display(book_IDs_and_narrators_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final cleanup and drop columns that are now in separate tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can now drop some columns now that we've normalized and stuff\n",
    "\n",
    "\n",
    "\n",
    "# rename columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import psycopg2\n",
    "def get_conn_cur():\n",
    "\n",
    " conn = psycopg2.connect(\n",
    "    host=\"final-project-db.cx6cm26umviq.us-east-1.rds.amazonaws.com\",\n",
    "    database=\"final_project_db\",\n",
    "    user=\"oseymour\",\n",
    "    password=DB_PASSWORD,\n",
    "    port='5432'\n",
    "    )\n",
    "\n",
    " cur = conn.cursor()\n",
    " return(conn, cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_query function\n",
    "def run_query(query_string):\n",
    "\n",
    "    conn, cur = get_conn_cur()  # get connection and cursor\n",
    "\n",
    "    cur.execute(query_string)  # executing string as before\n",
    "\n",
    "    my_data = cur.fetchall()  # fetch query data as before\n",
    "\n",
    "    # here we're extracting the 0th element for each item in cur.description\n",
    "    colnames = [desc[0] for desc in cur.description]\n",
    "\n",
    "    cur.close()  # close\n",
    "    conn.close()  # close\n",
    "\n",
    "    return (colnames, my_data)  # return column names AND data\n",
    "\n",
    "\n",
    "# Column name function for checking out what's in a table\n",
    "def get_column_names(table_name):  # argument of table_name\n",
    "    conn, cur = get_conn_cur()  # get connection and cursor\n",
    "\n",
    "    # Now select column names while inserting the table name into the WERE\n",
    "    column_name_query = (\n",
    "        \"\"\"SELECT column_name FROM information_schema.columns\n",
    "    WHERE table_name = '%s' \"\"\"\n",
    "        % table_name\n",
    "    )\n",
    "\n",
    "    cur.execute(column_name_query)  # execute\n",
    "    my_data = cur.fetchall()  # store\n",
    "\n",
    "    cur.close()  # close\n",
    "    conn.close()  # close\n",
    "\n",
    "    return my_data  # return\n",
    "\n",
    "\n",
    "# Check table_names\n",
    "def get_table_names():\n",
    "    conn, cur = get_conn_cur()  # get connection and cursor\n",
    "\n",
    "    # query to get table names\n",
    "    table_name_query = \"\"\"SELECT table_name FROM information_schema.tables\n",
    "       WHERE table_schema = 'public' \"\"\"\n",
    "\n",
    "    cur.execute(table_name_query)  # execute\n",
    "    my_data = cur.fetchall()  # fetch results\n",
    "\n",
    "    cur.close()  # close cursor\n",
    "    conn.close()  # close connection\n",
    "\n",
    "    return my_data  # return your fetched results\n",
    "\n",
    "\n",
    "# make sql_head function\n",
    "def sql_head(table_name):\n",
    "    conn, cur = get_conn_cur()  # get connection and cursor\n",
    "\n",
    "    # Now select column names while inserting the table name into the WERE\n",
    "    head_query = \"\"\"SELECT * FROM %s LIMIT 5; \"\"\" % table_name\n",
    "\n",
    "    cur.execute(head_query)  # execute\n",
    "    colnames = [desc[0] for desc in cur.description]  # get column names\n",
    "    my_data = cur.fetchall()  # store first five rows\n",
    "\n",
    "    cur.close()  # close\n",
    "    conn.close()  # close\n",
    "\n",
    "    df = pd.DataFrame(data=my_data, columns=colnames)  # make into df\n",
    "\n",
    "    return df  # return\n",
    "\n",
    "\n",
    "# drop a table from your rdb (if you try to create a table that already exists, it'll throw an error)\n",
    "def my_drop_table(tab_name):\n",
    "    conn, cur = get_conn_cur()\n",
    "    tq = \"\"\"DROP TABLE IF EXISTS %s CASCADE;\"\"\" % tab_name\n",
    "    cur.execute(tq)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tables_loaded = []\n",
    "\n",
    "# drop the tables if they exist\n",
    "# for table in list_of_tables_loaded:\n",
    "# \tmy_drop_table(table)\n",
    "\n",
    "# make tables\n",
    "conn, cur = get_conn_cur()\n",
    "# scouting\n",
    "# create_query = \"\"\"CREATE TABLE scouting (ColonyEmiID varchar(100), behavior_timestamp timestamp, Who varchar(30),\n",
    "# Location varchar(30), Nest varchar(30), X1stToEnter int, ArenaArea varchar(30));\"\"\"\n",
    "\n",
    "# cur.execute(create_query)\n",
    "# conn.commit()\n",
    "\n",
    "\n",
    "# # tandem\n",
    "# create_query = \"\"\"CREATE TABLE tandem (ColonyEmiID varchar(100), behavior_timestamp timestamp, Who varchar(30),\n",
    "# Role varchar(30), Destination varchar(30));\"\"\"\n",
    "\n",
    "# cur.execute(create_query)\n",
    "# conn.commit()\n",
    "\n",
    "\n",
    "# # videos\n",
    "# create_query = \"\"\"CREATE TABLE videos (VideoColonyEmiID varchar(50), ColonyEmiID varchar(100), \n",
    "# FileName varchar(50), FileNameNoExt varchar(50), RecordingStartTime timestamp, Length interval);\"\"\"\n",
    "\n",
    "# cur.execute(create_query)\n",
    "# conn.commit()\n",
    "\n",
    "# # transport\n",
    "# create_query = \"\"\"CREATE TABLE transport (ColonyEmiID varchar(100), behavior_timestamp timestamp, \n",
    "# Who varchar(30), WhatsBeingCarried varchar(30), WhoIsCarried varchar(30), NestCarriedFrom varchar(30),\n",
    "# NestCarriedTo varchar(30));\"\"\"\n",
    "\n",
    "# cur.execute(create_query)\n",
    "# conn.commit()\n",
    "\n",
    "\n",
    "# # colonies_and_emigrations\n",
    "# create_query = \"\"\"CREATE TABLE colonies_and_emigrations (ColonyEmiID varchar(100),\n",
    "# Colony varchar(30), EmiNum int, EmiNotes varchar(50));\"\"\"\n",
    "\n",
    "# cur.execute(create_query)\n",
    "# conn.commit()\n",
    "\n",
    "# cur.close()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: have to do queries!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
